{
 "metadata": {
  "language": "Julia",
  "name": "astro585_hw1_stefansson"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "#Homework 1: Astro 585 - Gu\u00f0mundur K\u00e1ri Stef\u00e1nsson, Astrophysics\n\n#Question 1: - Setting up Julia\n\n###Answer:\nI set up Julia (compiled from source) on my own laptop running Arch-Linux:\n\n__Moreover, I got the following interfaces to work:__ \n\n1) **IJulia notebook** - _I like this the best_\n\n2) **IJulia qtconsole** - _this seems like a good option; a console interface that supports inline graphics_\n\n3) **Julia console** - _the lowest level one_\n\nFor this homework I'm going with the first option - I like the clean cut interface."
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "#Question 2: Mean values\n\n##2a) Generating simulated data\n###Answer:\nIssuing the following commands:"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "srand(42);\nN=10000;\ntrue_mean=10000.;\ny = true_mean+randn(N);",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "sample_mean = mean(y);\nsample_var = var(y);\n(sample_mean,sample_var)",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": "(9999.998011659798,1.0199227833780644)"
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "The touple above gives the calculated sample mean, and sample variance."
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "##2b) Exploring floating point arithmetics\n###Answer:"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "y32bit = convert(Array{Float32,1},y);\ny16bit = convert(Array{Float16,1},y);",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "**For 32bits we get**"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "sample_mean32bit = mean(y32bit);\nsample_var32bit = var(y32bit);\n(sample_mean32bit,sample_var32bit)",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": "(9999.998f0,1.0199206f0)"
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "**For 16 bits we get**"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "sample_mean16bit = mean(y16bit);\nsample_var16bit = var(y16bit);\n(sample_mean16bit,sample_var16bit)",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 13,
       "text": "(Inf32,Inf32)"
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "We see that they have a large difference. The 16bits we can't even finish the computation! This makes sense as the highest number you can achieve with 16 bits is 2^16 = 65536, and I guess the mean function adds all the y16bit values togeather before dividing. The most accurate one is the one given in 2a) which uses 64 bits, in 32bits we see less precision."
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "##2c) Hypothesizing\n\n**Increasing N to 10^5**\n\nIf we increase N to 10^5 I think the 32bit method will still work, and it will give a similar _sample_mean32bit_ value, and similar _sample_var32bit_ value.\n\nThe 16bit method still won't work - it will continue to give Inf for both _sample_mean16bit_ and _sample_var16bit_.\n\n**Increasing _true_mean_ to 10^5 (and N=10^4)**\n\nThis will increase the _sample_mean32bit_ by rougly an order of magnitude, but _sample_var32bit_ will still be roughly the same.\n\nThe 16bit method still won't work - the cumulative sum of the N=10000 numbers will be too much!\n\n"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "##2d) Testing my hypotheses\n\n**Increasing N to 10^5**"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "N = 100000;\ny = true_mean+randn(N);\nsample_mean = mean(y);\nsample_var = var(y);\ny32bit = convert(Array{Float32,1},y);\ny16bit = convert(Array{Float16,1},y);\nsample_mean32bit = mean(y32bit);\nsample_var32bit = var(y32bit);\nsample_mean16bit = mean(y16bit);\nsample_var16bit = var(y16bit);",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "**_For 32bits we get:_**"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "(sample_mean32bit,sample_var32bit)",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 16,
       "text": "(9999.995f0,0.99714255f0)"
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "_-- Similar to the result above - like expected!_"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "*__For 16bits we get:__*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "(sample_mean16bit,sample_var16bit)",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 17,
       "text": "(Inf32,Inf32)"
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "_-- Still INF - like expected!_"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "##2e) Lessons learned\n\nYou have to be careful with precision when working with variables. Be aware of overflows!"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "#Question 3 - Variance algorithms\n\nLets define our _var_onepass_ and _var_twopass_ functions to calculate the variance of a given array _y_:\n\n##3a) One pass function:"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "function var_onepass(y::Array)\n    n = length(y);\n    sum = zero(y[1]);\n    sum2 = zero(y[1]);\n    for i in 1:n\n        sum = sum + y[i];\n        sum2 = sum2 + y[i]*y[i];\n    end\n    mean = sum/n;\n    return (1/(n-1))*(sum2-n*mean^2);\nend",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 16,
       "text": "var_onepass (generic function with 1 method)"
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "##3b) Two pass function:"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "function var_twopass(y::Array)\n    n = length(y);\n    sum = zero(y[1]);\n    diffsum2 = zero(y[1]);\n    #First pass\n    for i in 1:n\n        sum = sum + y[i];\n    end\n    mean = sum/n;\n    #Second pass\n    for i in 1:n\n        diffsum2 = diffsum2 + (y[i]-mean)*(y[i]-mean);\n    end\n    return (1/(n-1))*diffsum2;\nend\n        ",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 177,
       "text": "var_twopass (generic function with 1 method)"
      }
     ],
     "prompt_number": 177
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "##3c) Comparing the accuracy between the two functions:\nLets define the variables needed:"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "N = 1000000;\ntrue_mean = 1000000.;\ny = true_mean+randn(N);",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "For _var_onepass_ we get:"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "varianceOnePass = var_onepass(y)",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 17,
       "text": "0.9953289953289952"
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "For _var_onepass_ we get:"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "varianceTwoPass = var_twopass(y)",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 18,
       "text": "0.9974874778586887"
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Both are quite the same, but we expect the value closer to one to be more accurate (the _twopass_ one), as we have a million random numbers, so we expect to have a well sampled gaussian distribution.\n\n#3 d) When to use the one-pass algorithm, and when the two-pass?\n\nIf we have large numbers compared to the precision being used then we have to be careful about the onepass algorithm, as it then has some hidden numerical problems: the sums of the squares $\\sum_{i=i}^n a_i^2 $ is very close to $n\\bar{a}^2$ so when we subtract them, we get big roundoff errors. The twopass method also has this, but the effect is smaller as we are deling with smaller numbers. \n\nLets check this by changing the precision:"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "y32bit = convert(Array{Float32,1},y);",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "var32bit_onePass = var_onepass(y32bit)",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 23,
       "text": "1.83526541266307e10"
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "var32bit_twoPass = var_twopass(y32bit)",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 24,
       "text": "1.2656995017059015e7"
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Here we see some catastrophic cancellations using both functions - the cancellation originates in the lack of precision (see below). Both algorithms add all the number together in their respective _sum_ (or _sum2_) variables. In both cases we see the catastrophic effect of roundoff errors in the big number they work with, as well as big cancellation errors due to subtraction. The _two pass_ algorithm, however, performs marginally better, but at the expense of being longer to calculate (two for loops). So which algorithm to choose depends on if you want better precision and more correct answer, or to be fast."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "y32bit[1:10] #Very low precision for this problem!",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 31,
       "text": "10-element Array{Float32,1}:\n 999999.0  \n      1.0e6\n      1.0e6\n      1.0e6\n      1.0e6\n      1.0e6\n      1.0e6\n      1.0e6\n 999998.0  \n      1.0e6"
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "##3e) The third, \"on-line\" algorithm for sample variance:"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "function var_online(y::Array)\n  n = length(y);\n  sum1 = zero(y[1]);\n  mean = zero(y[1]);\n  M2 = zero(y[1]);\n  for i in 1:n\n\t  diff_by_i = (y[i]-mean)/i;\n\t  mean = mean +diff_by_i;\n\t  M2 = M2 + (i-1)*diff_by_i*diff_by_i+(y[i]-mean)*(y[i]-mean); \n  end;  \n  variance = M2/(n-1);\nend",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 28,
       "text": "var_online (generic function with 1 method)"
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "We see that this algorithm never substracts large numbers (y[i]-mean)/i and there is no _sum_ variable that accumulates into a huge number were roundoff error could also accumulate. We therefore predict that this algorithm would perform much better in the _32bit_-test case we tested above.\n\nFirst lets try the 64bit precision:"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "var_online(y) #Looks good!",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 29,
       "text": "0.9974874778289666"
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "var_online(y32bit) #Good results!",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 32,
       "text": "1.0132802f0"
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "We see that this algorithm does indeed perform much better with this low 32bit precision!"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "#Question 4. Computing probabilities"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "$$p(y_i | z_i) = \\frac{1}{\\sqrt{2\\pi \\sigma_i^2}} e^{(y_i-z_i)^2/(2\\sigma_i^2)}  $$\n\nNow we consider a radial velocity planet search that measures the velocity of a target star with a precision of $\\sigma_i = 2$m/s at each of $N_{obs}$ well separated observation times. In the simplest possible model is that the star has no planets and it\u2019s true velocity is a constant 0 m/s.\n\n##4a) Simulate measurements\n\nThen assuming the star has no planets, we create the following arrays:"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "srand(42);\nN = 100; #Number of observations\ntrue_velocity = zeros(N); # star's true velocities\nprecisions = 2*ones(N); # Measurement uncertainties\nmeasured_velocity = true_velocity + 2*randn(N); #Simulated observations; they have a std. of 2",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 122,
       "text": "100-element Array{Float64,1}:\n -1.11205  \n -0.888767 \n  0.0543107\n -0.598968 \n  3.55572  \n -2.2898   \n -0.937212 \n  0.312287 \n -5.28398  \n  2.00662  \n  2.16476  \n  0.374056 \n  1.0363   \n  \u22ee        \n -1.86099  \n -3.98111  \n  0.27508  \n  6.29915  \n -1.44282  \n -1.15419  \n  0.918768 \n  0.494415 \n -0.051325 \n -1.26416  \n -2.04084  \n -2.62302  "
      }
     ],
     "prompt_number": 122
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "##4b) A function to calculate the probability of a single measurement value (in Arrays)"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "function calc_prob(y::Array,sigma::Array,z::Array) # Calculates the probability of y_i, given sigma and z in each row\n    myConst = 1/sqrt(2*pi*sigma.*sigma);\n    exponent = -((y-z).*(y-z))./(2*sigma.*sigma); #Note the minus - it was missing from the problem description!!\n    return myConst.*exp(exponent)\nend",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 152,
       "text": "calc_prob (generic function with 2 methods)"
      }
     ],
     "prompt_number": 152
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "##4c) A function to calculate the probability of arrays of values"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "function calc_probset(y::Array,sigma::Array,z::Array)\n    #First calculate each indihvidual probability using the function above\n    probIndiv = calc_prob(y,sigma,z);\n    #Assuming all observations are independent, and uncorrelated,\n    #the overall probability will be the product of the individual ones\n    return prod(probIndiv)\nend",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 158,
       "text": "calc_probset (generic function with 2 methods)"
      }
     ],
     "prompt_number": 158
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "##4d) Testing the function in 4c)\n\nFor $N_{obs} = 100$ we get:"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "srand(42);\nN = 100; #Number of observations\ntrue_velocity = zeros(N); # star's true velocities\nprecisions = 2*ones(N); # Measurement uncertainties\nmeasured_velocity = true_velocity + 2*randn(N); #Simulated observations; they have a std. of 2\nn100prob = calc_probset(measured_velocity,precisions,true_velocity)",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 168,
       "text": "5.506290709100701e-95"
      }
     ],
     "prompt_number": 168
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "\nFor $N_{obs} = 600$ we get:"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "srand(42);\nN = 600; #Number of observations\ntrue_velocity = zeros(N); # star's true velocities\nprecisions = 2*ones(N); # Measurement uncertainties\nmeasured_velocity = true_velocity + 2*randn(N); #Simulated observations; they have a std. of 2\nn600prob = calc_probset(measured_velocity,precisions,true_velocity)",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 167,
       "text": "0.0"
      }
     ],
     "prompt_number": 167
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Both results seem plausible: In the former case we are multiplying 100 numbers of the magnitude 0.1 together, so we expect an answer close to the order of $10^{-100}$ (we are pretty close). For the latter we similarly would expect a probability around the order of $10^{-600}$ - however, we see that we have run into underflow errors.\n\n##4e) Log likelihoods"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Taking the log of the results above:"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "n100log_prob = log(n100prob)\nn600log_prob = log(n600prob)\nprintln(n100log_prob);\nprint(n600log_prob);",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "-217"
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": ".03969263050607\n-Inf"
      }
     ],
     "prompt_number": 173
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Lets write another function, that works with log-likelihoods instead:"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "function logcalc_probset(y::Array,sigma::Array,z::Array)\n    #First calculate each individual probability using the function above\n    logprobIndiv = log(calc_prob(y,sigma,z));\n    #Assuming all observations are independent, and uncorrelated,\n    #the overall probability will be the product of the individual ones\n    return sum(logprobIndiv)\nend",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 164,
       "text": "logcalc_probset (generic function with 1 method)"
      }
     ],
     "prompt_number": 164
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Comparing with the log-results above"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "srand(42);\nN = 100; #Number of observations\ntrue_velocity = zeros(N); # star's true velocities\nprecisions = 2*ones(N); # Measurement uncertainties\nmeasured_velocity = true_velocity + 2*randn(N); #Simulated observations; they have a std. of 2\nn100log_prob2 = logcalc_probset(measured_velocity,precisions,true_velocity)\n\nN = 600; #Number of observations\ntrue_velocity = zeros(N); # star's true velocities\nprecisions = 2*ones(N); # Measurement uncertainties\nmeasured_velocity = true_velocity + 2*randn(N); #Simulated observations; they have a std. of 2\nn600log_prob2 = logcalc_probset(measured_velocity,precisions,true_velocity)\n\n#Working in logspace\nprintln(\"Results from log-space\");\nprintln(n100log_prob2);\nprintln(n600log_prob2);\nprintln();\n\nprintln(\"Compared with results from non-log-space\");\nprintln(n100log_prob);\nprint(n600log_prob);\n",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "Results from log-space\n"
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "-217.03969263050607\n-1283.3263636300205\n\nCompared with results from non-log-space\n-217.03969263050607\n-Inf"
      }
     ],
     "prompt_number": 176
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "##4f) Lessons learned\n\nWe see that the results for 100 results are similar, but we have to work in log-space for 600 observations. So in general, if we are calculating probabilities for large datasets - we should be working in log-space"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "#Question 5: Improving a given function\n\n**1st suggestion** I would use a threshold value for checking the while-loop conditions: i.e. _abs(a-b) < eps_, where eps is a small number\n\n**2nd suggestion** I would make it check if _x_ is either 0 or negative and throw an error if so, because otherwise the program will run into an infinity loop\n\n**3rd suggestion** We have to be careful of roundoff errors, as the program might keep deviding or multiply by ten - we rather might want to work in log-space instead."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "",
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}